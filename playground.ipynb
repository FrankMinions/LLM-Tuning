{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/sentiment_comp_qaie_pairs.pkl','rb') as f:\n",
    "    pairs = pickle.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12325\n",
      "12272\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['智通财经APP讯，太平洋网络(00543)发布公告，将于2023年6月12日派发截至2022年12月31日止年度的末期股息每股0.1元人民币。\\n---\\n请从上文中抽取出所有公司/机构、对应的在本文中的情感倾向（积极、消极、中性）以及原因。\\n并用这样的格式返回：\\n{\"ORG\":..., \"sentiment\":..., \"reason\":...}',\n",
       " '{\"ORG\": \"太平洋网络\", \"sentiment\": \"积极\", \"reason\": \"宣布派发股息每股0.1元人民币\"}']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(pairs))\n",
    "pairs = [p for p in pairs if p[1] not in ['无','']]\n",
    "print(len(pairs))\n",
    "\n",
    "pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('data/sentiment_comp_ie.json','w',encoding='utf8') as f:\n",
    "    for p in pairs:\n",
    "        line = {\"q\":p[0],\"a\":p[1]}\n",
    "        f.write(json.dumps(line,ensure_ascii=False))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打乱顺序重新排列\n",
    "import json\n",
    "import random\n",
    "with open('data/sentiment_comp_ie_shuffled.json','w',encoding='utf8') as f:\n",
    "    random_pairs = random.sample(pairs,k=len(pairs))\n",
    "    for p in random_pairs:\n",
    "        line = {\"q\":p[0],\"a\":p[1]}\n",
    "        f.write(json.dumps(line,ensure_ascii=False))\n",
    "        f.write('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baichuan-7B inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/gby/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 下载百川大模型看看\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"baichuan-inc/baichuan-7B\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"baichuan-inc/baichuan-7B\", device_map=\"auto\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaiChuanForCausalLM(\n",
       "  (model): Model(\n",
       "    (embed_tokens): Embedding(64000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x DecoderLayer(\n",
       "        (self_attn): Attention(\n",
       "          (W_pack): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=64000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(model, \"weights/sentiment_comp_ie_shuffled_baichuan-7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"ChatGPT的提出对谷嘎、万度的搜索业务产生巨大打击，传统搜索引擎的作用性降低了。\n",
    "与此同时，OChat，Linguo等新兴语义搜索公司，迅速推出自己的类ChatGPT模型，并结合进自家搜索引擎，受到了很多用户的青睐。\n",
    "腾势、艾里等公司表示会迅速跟进ChatGPT和AIGC的发展，并预计在年底前推出自己的大模型。\n",
    "大型图片供应商视觉中国称ChatGPT对公司业务暂无影响，还在观望状态。\n",
    "（本文图片来自视觉中国，上观新闻为您报道。）\n",
    "更多报道：\n",
    "- 亚牛逊公司关于AIGC的表态\n",
    "- 巨硬公司昨日在A股上市\n",
    "---\n",
    "请从上文中抽取出所有公司，以及对应的在本文中的情感倾向（积极、消极、中性）以及原因。\n",
    "请用这样的格式返回：\n",
    "{\"ORG\":..., \"sentiment\":..., \"reason\":...}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "阿里巴巴集团。2014年9月19日，马云在纽约联合国总部宣布：“阿里巴巴将投入一千亿人民币设立社会公益基金会”。\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "streamer = TextStreamer(tokenizer,skip_prompt=True,skip_special_tokens=True)\n",
    "\n",
    "inputs = tokenizer(\"我们说的艾里巴巴公司，指的是\", return_tensors='pt')\n",
    "inputs = inputs.to('cuda:0')\n",
    "output = model.generate(**inputs, max_new_tokens=1024,repetition_penalty=1.1, streamer=streamer)\n",
    "# 模型非常自信！（类似于模型自动纠错的能力）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGLM inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.52s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel,AutoTokenizer\n",
    "device=torch.device(2)\n",
    "\n",
    "model_path = \"THUDM/chatglm-6b\"\n",
    "# model_glm = AutoModel.from_pretrained(model_path, trust_remote_code=True).half().to(device)\n",
    "model_glm = AutoModel.from_pretrained(model_path, trust_remote_code=True,device_map='auto')\n",
    "tokenizer_glm = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "# model_glm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load LoRA\n",
    "model_glm = PeftModel.from_pretrained(model_glm, \"weights/sentiment_comp_ie\").half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我不确定您所指的“艾里巴巴公司”是指哪个公司。如果您能提供更多上下文或信息,我将尽力回答您的问题。\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "streamer = TextStreamer(tokenizer_glm,skip_prompt=True,skip_special_tokens=True)\n",
    "\n",
    "inputs = tokenizer_glm(\"我们说的艾里巴巴公司，指的是\", return_tensors='pt')\n",
    "inputs = inputs.to('cuda:2')\n",
    "output = model_glm.generate(**inputs, max_new_tokens=1024,repetition_penalty=1.1, streamer=streamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "融资资金将用于完善生成式AI新引擎的构建。\n",
      "\n",
      "创业邦获悉，自然语言技术AI服务商竹间智能宣布已完成D2轮融资，由金浦投资、金库资本、江苏文投、隽赐资本等联合投资。至今，竹间智能已累计完成7轮融资，陆续引入科沃斯、云晖资本、中银国际等股东机构。值得一提的是，竹间智能已经正式推出运用类ChatGPT技术的成熟AIGC产品，并即将开启新一轮融资。\n",
      "\n",
      "融资资金将用于完善生成式AI新引擎的构建，把以ChatGPT为代表的大语言模型和AIGC技术全面融入竹间产品体系，结合大小模型构建双引擎驱动产品迭代和技术升级，平衡大小模型的优缺点，全面焕新产品功能和服务模式，并正式将产品同步推向海外市场，成为服务全球企业和用户的跨国NLP能力厂商。\n",
      "\n",
      "竹间智能由前微软（亚洲）互联网工程院副院长简仁贤于2015年创办，致力于以自然语言处理、情感计算、深度学习、知识工程、文本处理等人工智能技术为基础，将AI能力惠及千行百业。\n",
      "\n",
      "---\n",
      "请从上文中抽取出所有公司，以及对应的在本文中的情感倾向（积极、消极、中性）以及原因。\n",
      "请用这样的格式返回：\n",
      "{\"ORG\":..., \"sentiment\":..., \"reason\":...}\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizer自动把中文括号转成英文括号了...所以你直接在输出里面可能无法直接匹配到 prompt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatGLM2-6B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/gby/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:09<00:00,  1.33s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True, device='cuda')\n",
    "# model = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True, device_map='auto')\n",
    "# model = model.eval()\n",
    "# model\n",
    "# response, history = model.chat(tokenizer, \"你好\", history=[])\n",
    "# print(response)\n",
    "\n",
    "# response, history = model.chat(tokenizer, \"晚上睡不着应该怎么办\", history=history)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [00:11<00:00,  1.57s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'transformer.embedding': 0,\n",
       " 'transformer.rotary_pos_emb': 0,\n",
       " 'transformer.encoder.layers.0': 0,\n",
       " 'transformer.encoder.layers.1': 0,\n",
       " 'transformer.encoder.layers.2': 0,\n",
       " 'transformer.encoder.layers.3': 0,\n",
       " 'transformer.encoder.layers.4': 0,\n",
       " 'transformer.encoder.layers.5': 0,\n",
       " 'transformer.encoder.layers.6': 1,\n",
       " 'transformer.encoder.layers.7': 1,\n",
       " 'transformer.encoder.layers.8': 1,\n",
       " 'transformer.encoder.layers.9': 1,\n",
       " 'transformer.encoder.layers.10': 1,\n",
       " 'transformer.encoder.layers.11': 1,\n",
       " 'transformer.encoder.layers.12': 1,\n",
       " 'transformer.encoder.layers.13': 1,\n",
       " 'transformer.encoder.layers.14': 2,\n",
       " 'transformer.encoder.layers.15': 2,\n",
       " 'transformer.encoder.layers.16': 2,\n",
       " 'transformer.encoder.layers.17': 2,\n",
       " 'transformer.encoder.layers.18': 2,\n",
       " 'transformer.encoder.layers.19': 2,\n",
       " 'transformer.encoder.layers.20': 2,\n",
       " 'transformer.encoder.layers.21': 2,\n",
       " 'transformer.encoder.layers.22': 3,\n",
       " 'transformer.encoder.layers.23': 3,\n",
       " 'transformer.encoder.layers.24': 3,\n",
       " 'transformer.encoder.layers.25': 3,\n",
       " 'transformer.encoder.layers.26': 3,\n",
       " 'transformer.encoder.layers.27': 3,\n",
       " 'transformer.encoder.final_layernorm': 3,\n",
       " 'transformer.output_layer': 0}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hf_device_map\n",
    "model.hf_device_map['transformer.output_layer'] = model.hf_device_map['transformer.embedding']\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True, device_map=model.hf_device_map)\n",
    "model.hf_device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = tokenizer('你好',return_tensors='pt')\n",
    "res_glm = tokenizer_glm('你好',return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithPast(loss=tensor(13.0469, dtype=torch.float16, grad_fn=<ToCopyBackward0>), logits=tensor([[[-10.7578, -10.7578,   0.4590,  ..., -10.7578, -10.7578, -10.7578],\n",
       "         [-12.5938, -12.5938,   2.2852,  ..., -12.6016, -12.5938, -12.6016],\n",
       "         [-10.7031, -10.7031,   1.2051,  ..., -10.6953, -10.6953, -10.7031],\n",
       "         [ -9.9375,  -9.9375,   0.6338,  ...,  -9.9297,  -9.9297,  -9.9297]]],\n",
       "       dtype=torch.float16, grad_fn=<ToCopyBackward0>), past_key_values=((tensor([[[[-9.3164e-01,  3.4424e-01,  9.4580e-01,  ...,  1.5410e+00,\n",
       "           -2.2988e+00,  3.0000e+00],\n",
       "          [ 4.2700e-01, -3.5791e-01,  1.0029e+00,  ..., -2.6641e+00,\n",
       "            2.5391e+00, -1.4932e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 3.8849e-02, -2.1252e-01, -6.4514e-02,  ...,  1.2471e+00,\n",
       "           -3.5977e+00,  3.8574e+00],\n",
       "          [-8.3887e-01,  3.1055e+00, -2.1367e+00,  ..., -2.8008e+00,\n",
       "            2.4082e+00,  1.8213e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 5.9375e-01, -1.7065e-01,  1.5186e-01,  ...,  5.7770e-02,\n",
       "           -2.2520e+00,  2.6191e+00],\n",
       "          [-9.4922e-01,  4.7095e-01,  9.3018e-02,  ..., -1.3564e+00,\n",
       "            1.6602e+00, -6.1572e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 5.4590e-01,  6.6162e-01, -1.1873e-04,  ..., -4.0381e-01,\n",
       "           -3.2676e+00,  3.0078e+00],\n",
       "          [-1.1992e+00, -2.8516e-01,  1.6028e-01,  ..., -2.1504e+00,\n",
       "            1.8994e+00, -1.9238e-01]]]], dtype=torch.float16,\n",
       "       grad_fn=<ToCopyBackward0>), tensor([[[[ 0.0040, -0.0025, -0.0003,  ..., -0.0006, -0.1190,  0.0003],\n",
       "          [-0.0021, -0.0014, -0.0059,  ...,  0.0027,  0.0071, -0.0025]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0009,  0.0024, -0.0066,  ...,  0.0011,  0.1970,  0.0035],\n",
       "          [-0.0027,  0.0188,  0.0015,  ..., -0.0133, -0.0094,  0.0064]]],\n",
       "\n",
       "\n",
       "        [[[-0.0165, -0.0070,  0.0015,  ..., -0.0110,  0.0423, -0.0148],\n",
       "          [-0.0342, -0.0331,  0.0350,  ...,  0.0245, -0.0240, -0.0045]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0022,  0.0070, -0.0165,  ...,  0.0023,  0.1009, -0.0067],\n",
       "          [ 0.0019, -0.0323,  0.0646,  ...,  0.0226,  0.0358,  0.0126]]]],\n",
       "       dtype=torch.float16, grad_fn=<ToCopyBackward0>)), (tensor([[[[ 0.0807, -0.0089,  0.0204,  ..., -0.6646, -0.2627, -0.0939],\n",
       "          [-0.3333,  0.1328, -0.2539,  ...,  0.5425, -0.7788, -0.2771]]],\n",
       "\n",
       "\n",
       "        [[[-0.0238, -0.0645, -0.0427,  ..., -0.1196, -0.3103, -0.0260],\n",
       "          [-0.0101, -0.4211,  0.2964,  ...,  0.4106, -0.6416, -0.3184]]],\n",
       "\n",
       "\n",
       "        [[[-0.6548,  0.9624, -0.1898,  ...,  0.1053, -0.0148, -0.8369],\n",
       "          [-1.9648, -3.0117,  0.8359,  ...,  1.4365,  0.2356,  0.4065]]],\n",
       "\n",
       "\n",
       "        [[[-1.3984, -0.0132, -0.4888,  ..., -0.6772, -0.6768, -0.4832],\n",
       "          [ 2.4629, -4.5547, -0.2063,  ...,  0.3110,  1.8076,  0.8604]]]],\n",
       "       dtype=torch.float16, grad_fn=<ToCopyBackward0>), tensor([[[[-2.5959e-03,  3.2902e-04,  6.7482e-03,  ..., -3.1161e-04,\n",
       "           -4.8923e-04,  2.2354e-02],\n",
       "          [ 7.1764e-04,  2.0046e-03, -1.3895e-03,  ..., -2.3022e-03,\n",
       "            2.1210e-03, -1.1581e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 1.3647e-03, -3.6068e-03, -1.1368e-02,  ...,  1.0419e-04,\n",
       "            8.8348e-03,  3.8910e-02],\n",
       "          [-7.8440e-04,  2.5654e-03,  6.2790e-03,  ..., -9.2850e-03,\n",
       "            9.2163e-03, -1.9669e-02]]],\n",
       "\n",
       "\n",
       "        [[[-2.9907e-01, -2.0096e-02, -3.4937e-01,  ..., -1.7188e-01,\n",
       "           -3.5718e-01, -3.6713e-02],\n",
       "          [ 2.0981e-02, -1.2122e-01,  1.4453e-01,  ...,  9.2468e-02,\n",
       "           -1.4380e-01,  4.3384e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 2.0581e-01, -1.7151e-01, -1.2866e-01,  ...,  1.4099e-01,\n",
       "           -9.5459e-02, -3.6230e-01],\n",
       "          [-6.7993e-02,  9.2285e-02,  8.6182e-02,  ..., -1.0300e-02,\n",
       "           -8.8318e-02,  3.1250e-01]]]], dtype=torch.float16,\n",
       "       grad_fn=<ToCopyBackward0>)), (tensor([[[[-0.0053, -0.0127,  0.0870,  ..., -0.0861,  0.0861, -0.2134],\n",
       "          [-0.0302,  0.0429,  0.0230,  ...,  0.3252,  0.0311,  0.5933]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0580,  0.0113,  0.0375,  ..., -0.4036,  0.5601, -0.3181],\n",
       "          [-0.0033, -0.0319, -0.0176,  ...,  0.6348, -0.2861,  0.3247]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1455, -1.1084,  0.9692,  ..., -0.3386, -0.7222,  1.1172],\n",
       "          [-0.4619,  0.2617, -1.1064,  ...,  1.0713, -0.8853,  0.7251]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0566, -0.2483,  1.2783,  ..., -2.1133, -0.0127,  1.2588],\n",
       "          [-0.7495,  0.2025, -1.1875,  ...,  1.4463,  0.2834,  1.5508]]]],\n",
       "       dtype=torch.float16, grad_fn=<ToCopyBackward0>), tensor([[[[-0.0029, -0.0607,  0.0111,  ...,  0.0138, -0.0219,  0.0205],\n",
       "          [-0.0130,  0.0042,  0.0171,  ..., -0.0063,  0.0098, -0.0060]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0173,  0.0687, -0.0076,  ..., -0.0113,  0.0150, -0.0304],\n",
       "          [ 0.0348,  0.0167, -0.0425,  ...,  0.0065, -0.0269,  0.0241]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0779,  0.2341,  0.1620,  ...,  0.4146,  0.6499,  0.1052],\n",
       "          [ 0.4221, -0.1344,  0.1587,  ...,  0.6587,  0.1226,  0.2297]]],\n",
       "\n",
       "\n",
       "        [[[-0.0928,  0.0673, -0.1552,  ..., -0.4985, -0.4214, -0.2081],\n",
       "          [-0.1711, -0.3240, -0.1528,  ..., -0.3882, -0.2263, -0.3159]]]],\n",
       "       dtype=torch.float16, grad_fn=<ToCopyBackward0>)), (tensor([[[[-2.3193e-02, -5.2986e-03, -1.8097e-02,  ..., -1.4307e-01,\n",
       "            6.9702e-02, -7.1973e-01],\n",
       "          [ 8.1110e-04,  3.4676e-03, -2.0203e-02,  ..., -3.9575e-01,\n",
       "           -8.3203e+00,  4.0088e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.4515e-03, -5.1758e-02, -5.8777e-02,  ...,  3.2422e-01,\n",
       "           -4.0321e-03,  1.9028e-02],\n",
       "          [-1.7033e-03,  4.1626e-02, -1.8250e-02,  ...,  7.9004e-01,\n",
       "           -6.5312e+00,  1.6248e-01]]],\n",
       "\n",
       "\n",
       "        [[[-4.3970e-01, -6.5137e-01,  1.2705e+00,  ...,  6.0400e-01,\n",
       "           -2.7368e-01, -1.4199e+00],\n",
       "          [ 3.9160e-01, -6.0638e-02,  9.7656e-02,  ...,  1.8184e+00,\n",
       "           -2.6973e+00,  3.7549e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 3.6426e-01, -8.6133e-01,  2.6406e+00,  ...,  3.2153e-01,\n",
       "           -1.6240e+00, -6.6797e-01],\n",
       "          [ 6.9336e-01,  1.2756e-01,  8.0322e-01,  ..., -1.1270e+00,\n",
       "           -4.6289e-01, -7.1582e-01]]]], dtype=torch.float16,\n",
       "       grad_fn=<ToCopyBackward0>), tensor([[[[-0.0180,  0.0028,  0.0024,  ...,  0.0016, -0.0233, -0.0109],\n",
       "          [ 0.0023, -0.0085, -0.0028,  ..., -0.0047, -0.0040,  0.0237]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0195, -0.0017, -0.0130,  ..., -0.0196,  0.0564,  0.0335],\n",
       "          [ 0.0231,  0.0208, -0.0166,  ..., -0.0473,  0.0408, -0.0504]]],\n",
       "\n",
       "\n",
       "        [[[-0.1790, -0.1398,  0.0482,  ...,  0.5933,  0.2041,  0.0512],\n",
       "          [-0.2201, -0.0843,  0.1898,  ..., -0.3132, -0.3835, -0.0300]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2356, -0.1080, -0.2430,  ..., -0.1385,  0.5693,  0.3535],\n",
       "          [-0.2046, -1.0195,  0.4592,  ..., -0.4888, -0.4346,  0.0593]]]],\n",
       "       dtype=torch.float16, grad_fn=<ToCopyBackward0>)), (tensor([[[[-1.9867e-02, -1.7014e-02,  3.8986e-03,  ..., -1.1309e+00,\n",
       "           -1.0508e+00, -8.9893e-01],\n",
       "          [ 3.8513e-02, -1.8295e-02, -4.1542e-03,  ..., -7.2559e-01,\n",
       "            1.3611e-01,  6.5283e-01]]],\n",
       "\n",
       "\n",
       "        [[[-3.9154e-02, -1.6537e-03, -2.2324e-02,  ..., -8.2825e-02,\n",
       "           -6.4307e-01, -6.6846e-01],\n",
       "          [-9.4235e-05,  5.1300e-02,  6.6101e-02,  ...,  1.8408e-01,\n",
       "           -2.1301e-02,  8.9417e-02]]],\n",
       "\n",
       "\n",
       "        [[[-1.8652e-01, -4.7290e-01,  1.1859e-01,  ..., -1.4014e+00,\n",
       "            2.1504e+00, -9.5703e-01],\n",
       "          [-4.0552e-01,  8.2715e-01, -8.8196e-02,  ...,  5.4443e-01,\n",
       "            2.8101e-01,  1.1348e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 2.0203e-01, -6.3379e-01,  4.3652e-01,  ..., -3.5996e+00,\n",
       "            5.3711e-01, -1.9268e+00],\n",
       "          [-9.4482e-01,  5.7129e-01,  2.9077e-01,  ...,  8.6963e-01,\n",
       "           -1.5637e-01, -6.8457e-01]]]], dtype=torch.float16,\n",
       "       grad_fn=<ToCopyBackward0>), tensor([[[[ 3.1235e-02,  2.7256e-03,  8.8043e-03,  ..., -1.9165e-02,\n",
       "            1.6052e-02, -9.7427e-03],\n",
       "          [ 1.2871e-02, -8.9569e-03, -1.6357e-02,  ...,  2.0767e-02,\n",
       "           -1.9089e-02,  2.1194e-02]]],\n",
       "\n",
       "\n",
       "        [[[-6.6101e-02, -6.0844e-04, -1.5686e-02,  ...,  5.0079e-02,\n",
       "           -3.3905e-02,  1.4099e-02],\n",
       "          [-1.2962e-02,  2.8091e-02,  1.9653e-02,  ..., -2.8458e-02,\n",
       "            2.1469e-02, -2.0962e-03]]],\n",
       "\n",
       "\n",
       "        [[[-6.9971e-01,  1.3708e-01, -7.5012e-02,  ..., -1.2091e-01,\n",
       "            3.1982e-02,  6.0107e-01],\n",
       "          [-1.0162e-01, -1.3953e-01, -3.3325e-01,  ..., -1.5869e-01,\n",
       "           -2.2751e-02,  3.5474e-01]]],\n",
       "\n",
       "\n",
       "        [[[-4.4775e-01, -3.5828e-02, -3.6475e-01,  ...,  1.8848e-01,\n",
       "           -1.2042e-01, -6.9434e-01],\n",
       "          [ 8.5449e-02, -6.3721e-01, -3.3838e-01,  ..., -1.2754e+00,\n",
       "           -1.9397e-01,  6.7383e-02]]]], dtype=torch.float16,\n",
       "       grad_fn=<ToCopyBackward0>)), (tensor([[[[-6.1989e-03,  7.0190e-02,  1.4801e-03,  ..., -8.4424e-01,\n",
       "           -7.3779e-01,  9.6240e-01],\n",
       "          [ 4.7028e-02,  2.6428e-02,  5.7220e-06,  ..., -3.5391e+00,\n",
       "           -1.6895e-01,  6.7285e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 2.7176e-02,  3.4546e-02,  3.4180e-02,  ...,  3.6670e-01,\n",
       "            1.0615e+00,  4.3530e-01],\n",
       "          [-2.4124e-02,  2.0538e-02,  7.5562e-02,  ..., -3.9863e+00,\n",
       "           -4.3628e-01,  5.5615e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 3.4863e-01, -4.7729e-01, -1.0459e+00,  ...,  1.6931e-01,\n",
       "           -1.2231e-01,  9.3457e-01],\n",
       "          [ 1.1377e-01,  6.9287e-01, -9.1553e-01,  ..., -1.4248e+00,\n",
       "           -4.3408e-01, -1.7712e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 8.2568e-01, -7.1045e-02, -2.1133e+00,  ..., -4.2090e-01,\n",
       "           -4.6094e+00,  1.2168e+00],\n",
       "          [-1.2422e+00,  2.0924e-03, -4.1650e-01,  ...,  4.1089e-01,\n",
       "            1.4648e+00,  1.8945e-01]]]], dtype=torch.float16,\n",
       "       grad_fn=<ToCopyBackward0>), tensor([[[[-0.0159, -0.0057, -0.0035,  ...,  0.0103, -0.0136, -0.0143],\n",
       "          [ 0.0116,  0.0687,  0.0310,  ...,  0.0077, -0.0504, -0.0074]]],\n",
       "\n",
       "\n",
       "        [[[-0.0530, -0.0193,  0.1243,  ...,  0.0044,  0.0831,  0.0195],\n",
       "          [-0.0033, -0.0805, -0.0500,  ...,  0.0015,  0.0597, -0.0087]]],\n",
       "\n",
       "\n",
       "        [[[-0.2167,  0.0185,  0.1035,  ...,  0.1454, -0.3943,  0.0123],\n",
       "          [-0.3933,  0.0128, -0.0017,  ..., -0.2189,  0.3643,  0.4675]]],\n",
       "\n",
       "\n",
       "        [[[-0.9219, -0.2449,  0.5884,  ..., -0.8447, -0.3979, -0.0781],\n",
       "          [-0.3640, -0.5938, -1.0527,  ...,  0.0504, -0.6968,  0.5679]]]],\n",
       "       dtype=torch.float16, grad_fn=<ToCopyBackward0>)), (tensor([[[[ 7.2205e-02,  4.4281e-02,  7.7942e-02,  ...,  3.4033e-01,\n",
       "            8.3545e-01, -2.6291e-02],\n",
       "          [ 1.4275e-02, -1.0559e-01,  1.8406e-03,  ...,  5.1641e+00,\n",
       "            2.3223e+00,  2.1082e-01]]],\n",
       "\n",
       "\n",
       "        [[[-3.7750e-02, -4.9164e-02,  2.6779e-02,  ...,  3.1812e-01,\n",
       "           -2.4829e-01, -1.1260e+00],\n",
       "          [ 7.5195e-02, -2.2614e-02, -1.9058e-02,  ...,  5.1289e+00,\n",
       "            2.1738e+00, -2.4976e-01]]],\n",
       "\n",
       "\n",
       "        [[[-5.0098e-01,  3.3228e-01, -4.0698e-01,  ...,  2.9346e-01,\n",
       "           -5.4395e-01,  7.7734e-01],\n",
       "          [ 6.4355e-01, -1.1504e+00, -5.5127e-01,  ...,  4.2656e+00,\n",
       "            1.3506e+00,  4.3018e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.3682e+00, -4.5020e-01, -3.6279e-01,  ...,  1.8350e+00,\n",
       "            5.8154e-01, -6.2354e-01],\n",
       "          [ 1.5508e+00,  4.3457e-01,  8.9551e-01,  ...,  4.5742e+00,\n",
       "            2.6250e+00,  2.4062e+00]]]], dtype=torch.float16,\n",
       "       grad_fn=<ToCopyBackward0>), tensor([[[[ 4.5654e-02, -5.9082e-02, -1.7853e-02,  ..., -5.0537e-02,\n",
       "           -1.1131e-02,  1.2489e-02],\n",
       "          [-4.9400e-03, -6.0081e-03,  1.2131e-03,  ...,  5.9242e-03,\n",
       "           -5.1651e-03, -2.4662e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 2.5330e-03,  1.2158e-01,  2.6077e-02,  ...,  5.9662e-02,\n",
       "            6.6589e-02,  4.8645e-02],\n",
       "          [-5.0468e-03, -8.4106e-02,  1.8435e-03,  ..., -4.4373e-02,\n",
       "           -3.7659e-02,  2.9663e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 2.3438e-01,  3.8403e-01,  3.4424e-01,  ...,  3.0615e-01,\n",
       "           -6.8298e-02, -2.9517e-01],\n",
       "          [-1.5125e-01, -3.2446e-01,  7.2266e-02,  ...,  2.7657e-03,\n",
       "            4.7516e-02, -3.4546e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 3.3276e-01,  1.2891e-01,  4.0894e-01,  ...,  9.4531e-01,\n",
       "            5.9229e-01, -3.4619e-01],\n",
       "          [ 8.5449e-01,  9.8145e-02,  1.0284e-02,  ..., -1.7471e-02,\n",
       "            5.9605e-04, -2.1936e-01]]]], dtype=torch.float16,\n",
       "       grad_fn=<ToCopyBackward0>)), (tensor([[[[-3.7048e-02,  1.9852e-02, -8.3971e-04,  ..., -3.2104e-01,\n",
       "            5.0293e-01, -3.6890e-01],\n",
       "          [ 2.5314e-02,  6.3232e-02,  1.1803e-02,  ..., -2.1309e+00,\n",
       "           -1.1211e+00, -8.6475e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 7.5073e-03, -9.1492e-02,  3.5583e-02,  ...,  2.2876e-01,\n",
       "            2.1594e-01,  1.4534e-02],\n",
       "          [-7.5195e-02, -7.3059e-02,  4.8859e-02,  ..., -1.1689e+00,\n",
       "            7.2559e-01, -4.1772e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 4.1528e-01,  1.1969e-01, -1.7273e-01,  ..., -5.7812e-01,\n",
       "            5.9082e-01, -2.8198e-01],\n",
       "          [-2.7393e-01, -3.8330e-01,  2.1484e-01,  ..., -5.7715e-01,\n",
       "            4.2700e-01,  1.7188e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 4.9414e-01, -2.2913e-01, -6.4453e-01,  ...,  2.2632e-01,\n",
       "           -2.3340e+00, -1.0938e+00],\n",
       "          [-1.1963e-01, -7.0850e-01, -8.1201e-01,  ...,  8.2642e-02,\n",
       "            4.6655e-01, -1.9541e+00]]]], dtype=torch.float16,\n",
       "       grad_fn=<ToCopyBackward0>), tensor([[[[ 6.5002e-02, -2.1942e-02,  1.2779e-02,  ..., -1.5662e-01,\n",
       "           -1.7975e-02, -3.5797e-02],\n",
       "          [ 3.9940e-03, -9.6130e-03, -2.2354e-03,  ...,  8.7585e-03,\n",
       "           -1.7075e-02,  3.0045e-02]]],\n",
       "\n",
       "\n",
       "        [[[-7.8552e-02,  6.6406e-02, -3.9337e-02,  ...,  1.5698e-01,\n",
       "            1.0429e-02,  6.8542e-02],\n",
       "          [-1.6663e-01, -1.5419e-02,  1.5221e-03,  ..., -2.7603e-02,\n",
       "           -7.0984e-02, -3.1226e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 7.2388e-02,  5.7220e-02,  2.9199e-01,  ...,  1.9800e-01,\n",
       "           -1.8115e-01, -5.9521e-01],\n",
       "          [-2.2217e-02, -2.7588e-01,  9.1910e-05,  ...,  1.5027e-01,\n",
       "            2.7979e-01, -1.4746e-01]]],\n",
       "\n",
       "\n",
       "        [[[-7.6318e-01,  1.1084e+00, -7.0007e-02,  ...,  2.3560e-01,\n",
       "            1.1353e-01, -6.2891e-01],\n",
       "          [ 3.7549e-01, -9.1980e-02,  1.6101e-01,  ..., -3.3276e-01,\n",
       "            4.6021e-01, -1.0052e-01]]]], dtype=torch.float16,\n",
       "       grad_fn=<ToCopyBackward0>)), (tensor([[[[-0.0112,  0.0788,  0.0443,  ..., -0.4216,  1.1367,  0.3440],\n",
       "          [ 0.0740, -0.0079, -0.0060,  ..., -0.1394, -0.5874, -0.0684]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0117, -0.0316, -0.0420,  ...,  0.6938,  0.9922, -0.6509],\n",
       "          [-0.0295, -0.0894, -0.0183,  ..., -0.2571,  0.1321, -0.1316]]],\n",
       "\n",
       "\n",
       "        [[[-0.4719, -0.6816, -0.1824,  ...,  2.3125,  1.0186, -0.3494],\n",
       "          [ 0.5859, -0.0319,  0.2542,  ..., -0.9053, -0.3472, -0.4727]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0410, -0.2888,  0.7300,  ...,  3.7832,  2.2207, -0.9883],\n",
       "          [ 0.2268,  0.7856,  0.6089,  ..., -2.7695,  0.7168,  0.9717]]]],\n",
       "       dtype=torch.float16, grad_fn=<ToCopyBackward0>), tensor([[[[-4.8752e-03, -4.0245e-03,  1.2457e-05,  ...,  2.1942e-02,\n",
       "            1.0506e-02,  1.4145e-02],\n",
       "          [-3.8483e-02, -5.7007e-02, -1.3481e-02,  ...,  4.3732e-02,\n",
       "            6.4148e-02, -2.8198e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 1.9932e-03, -1.3695e-03, -1.5125e-01,  ...,  3.5858e-02,\n",
       "           -2.4567e-02,  8.2016e-03],\n",
       "          [ 8.8074e-02,  9.5520e-02,  8.5632e-02,  ..., -1.2061e-01,\n",
       "           -1.6138e-01,  1.1542e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.2299e-02,  1.2244e-01,  3.1143e-02,  ...,  1.5601e-01,\n",
       "           -3.1616e-01, -1.6577e-01],\n",
       "          [ 1.3208e-01, -1.2708e-01, -4.8779e-01,  ..., -5.4321e-02,\n",
       "            4.4159e-02,  1.3684e-01]]],\n",
       "\n",
       "\n",
       "        [[[-4.0356e-01, -5.5029e-01,  1.4307e-01,  ...,  1.0752e+00,\n",
       "           -4.3921e-01, -3.7158e-01],\n",
       "          [ 8.2178e-01,  6.3281e-01, -1.3574e-01,  ..., -4.6729e-01,\n",
       "            2.2229e-01, -6.9531e-01]]]], dtype=torch.float16,\n",
       "       grad_fn=<ToCopyBackward0>)), (tensor([[[[ 9.5444e-03, -1.0315e-02,  1.6541e-02,  ...,  5.4492e-01,\n",
       "            9.3203e+00,  1.7896e-01],\n",
       "          [ 6.6040e-02, -1.9226e-02,  1.1833e-02,  ..., -8.8438e+00,\n",
       "            8.6426e-01, -1.8677e-01]]],\n",
       "\n",
       "\n",
       "        [[[-2.0416e-02,  1.1127e-01, -6.3293e-02,  ...,  3.4009e-01,\n",
       "            9.2109e+00,  9.6436e-02],\n",
       "          [ 9.5398e-02, -3.6163e-02, -4.8462e-02,  ..., -9.2109e+00,\n",
       "           -1.1260e+00, -9.4788e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 1.6943e-01, -6.8726e-02, -2.0485e-03,  ..., -1.7810e-01,\n",
       "            7.5078e+00, -1.8176e-01],\n",
       "          [-8.1299e-02, -3.8525e-01,  2.0715e-01,  ..., -6.5000e+00,\n",
       "           -2.4160e+00, -6.2027e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 5.8008e-01, -4.0894e-01, -9.0283e-01,  ...,  6.3281e-01,\n",
       "            4.0508e+00,  8.8135e-01],\n",
       "          [-2.1558e-01,  1.0089e-01,  4.4312e-01,  ..., -1.7334e+00,\n",
       "            1.7539e+00,  1.2539e+00]]]], dtype=torch.float16,\n",
       "       grad_fn=<ToCopyBackward0>), tensor([[[[-0.0084,  0.0314, -0.0154,  ...,  0.0107,  0.0350,  0.0164],\n",
       "          [ 0.0079, -0.0352, -0.0650,  ...,  0.0103, -0.0156, -0.0099]]],\n",
       "\n",
       "\n",
       "        [[[-0.0364,  0.0303,  0.0825,  ...,  0.1315, -0.0506,  0.0387],\n",
       "          [-0.0310,  0.0927,  0.1350,  ..., -0.0159, -0.0374,  0.0150]]],\n",
       "\n",
       "\n",
       "        [[[-0.5410,  0.5503, -0.1157,  ..., -0.1255, -0.4338,  0.4036],\n",
       "          [ 0.0208,  0.3201,  0.3425,  ..., -0.2231,  0.2024, -0.0950]]],\n",
       "\n",
       "\n",
       "        [[[-0.4446,  1.5215,  0.3442,  ..., -0.8740, -0.5098,  0.1888],\n",
       "          [-0.2369,  1.2715, -0.5400,  ...,  0.1804,  0.9980,  1.0352]]]],\n",
       "       dtype=torch.float16, grad_fn=<ToCopyBackward0>)), (tensor([[[[-2.2461e-02,  3.2318e-02,  1.3695e-02,  ...,  4.4897e-01,\n",
       "           -5.9766e-01,  2.3157e-01],\n",
       "          [ 4.9164e-02, -1.7090e-02, -2.6302e-03,  ...,  1.4287e+00,\n",
       "            2.8223e-01,  6.7383e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.8568e-03,  5.8794e-04, -8.2642e-02,  ..., -5.4688e-01,\n",
       "           -5.5371e-01, -1.8274e-01],\n",
       "          [-5.4474e-02, -2.1149e-02, -8.8440e-02,  ...,  6.7041e-01,\n",
       "            1.7676e+00,  1.0712e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.8457e-01,  1.7383e-01, -9.6680e-02,  ..., -7.5732e-01,\n",
       "           -4.5190e-01, -9.1797e-01],\n",
       "          [ 1.9470e-01,  1.5930e-01, -4.4141e-01,  ..., -4.3872e-01,\n",
       "            1.1143e+00, -1.0801e+00]]],\n",
       "\n",
       "\n",
       "        [[[-3.3813e-01,  3.7402e-01,  2.6440e-01,  ..., -1.9600e+00,\n",
       "            8.4668e-01, -8.9844e-01],\n",
       "          [-6.9336e-02,  4.0894e-01, -8.1689e-01,  ...,  7.3193e-01,\n",
       "           -2.6880e-01,  2.2998e-01]]]], dtype=torch.float16,\n",
       "       grad_fn=<ToCopyBackward0>), tensor([[[[ 0.0102,  0.0318,  0.0108,  ...,  0.0099,  0.0978,  0.0100],\n",
       "          [ 0.0216,  0.0138, -0.0187,  ..., -0.0414,  0.0122, -0.0048]]],\n",
       "\n",
       "\n",
       "        [[[-0.1053, -0.0527, -0.1345,  ..., -0.1307, -0.3425, -0.0525],\n",
       "          [-0.3164,  0.0147, -0.0230,  ...,  0.1550, -0.3433,  0.1147]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1903, -0.0454,  0.1080,  ..., -0.2346,  0.0412,  0.1970],\n",
       "          [ 0.0464,  0.0808, -0.0273,  ...,  0.0825, -0.3589,  0.2096]]],\n",
       "\n",
       "\n",
       "        [[[ 0.3152, -0.1025,  0.4758,  ...,  0.3591,  0.7363,  0.3850],\n",
       "          [ 0.3276,  0.3965,  0.2388,  ...,  0.9795, -0.3115,  0.4172]]]],\n",
       "       dtype=torch.float16, grad_fn=<ToCopyBackward0>)), (tensor([[[[ 0.0395, -0.0036, -0.0138,  ..., -0.3928, -0.2844, -1.2090],\n",
       "          [-0.0450,  0.0243,  0.0103,  ..., -0.1473, -0.0590,  0.0173]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1223, -0.0256,  0.0754,  ..., -0.7808,  0.0747, -1.3037],\n",
       "          [-0.0446, -0.0174,  0.0300,  ..., -0.2988,  1.2783,  0.0199]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2279,  0.0218, -0.0076,  ..., -0.0825, -0.2478, -0.0748],\n",
       "          [ 0.0673,  0.4077,  0.2164,  ...,  0.1189,  1.1631, -0.3735]]],\n",
       "\n",
       "\n",
       "        [[[-0.1658,  0.3699,  0.3281,  ..., -0.4390, -2.4668,  0.8403],\n",
       "          [-0.2510,  0.1583, -0.5562,  ..., -1.7822,  1.6660,  1.1650]]]],\n",
       "       dtype=torch.float16, grad_fn=<ToCopyBackward0>), tensor([[[[-0.1194,  0.0029,  0.0078,  ..., -0.0839,  0.0726,  0.0061],\n",
       "          [ 0.0562, -0.0189, -0.0272,  ...,  0.0029,  0.0412,  0.0781]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2549, -0.0569, -0.1191,  ...,  0.1028, -0.1887,  0.0171],\n",
       "          [-0.0194, -0.0825,  0.1968,  ..., -0.3120,  0.2360,  0.4045]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1242, -0.0048,  0.4658,  ...,  0.2729, -0.1171,  0.1989],\n",
       "          [-0.0574,  0.0446,  0.0298,  ...,  0.1232,  0.1837,  0.2231]]],\n",
       "\n",
       "\n",
       "        [[[ 1.3721,  0.9180,  0.4543,  ...,  0.0160, -0.0732,  0.6094],\n",
       "          [-0.5986, -0.2230, -0.0950,  ...,  0.2610, -0.4026, -0.0073]]]],\n",
       "       dtype=torch.float16, grad_fn=<ToCopyBackward0>)), (tensor([[[[ 6.2683e-02, -4.4174e-03,  2.6684e-03,  ..., -4.8242e-01,\n",
       "            1.8094e+01, -7.5488e-01],\n",
       "          [ 4.9713e-02,  5.2704e-02,  4.1351e-02,  ...,  2.2383e+00,\n",
       "           -3.4688e+00, -1.1738e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 3.9001e-02,  6.5063e-02, -1.3878e-02,  ...,  9.1260e-01,\n",
       "            1.8656e+01,  6.2402e-01],\n",
       "          [-2.8870e-02, -1.4458e-02, -5.3482e-03,  ...,  3.0449e+00,\n",
       "           -3.1523e+00, -4.5728e-01]]],\n",
       "\n",
       "\n",
       "        [[[-9.3079e-02, -4.3945e-01, -1.2952e-01,  ...,  1.2140e-01,\n",
       "            1.0883e+01,  1.3945e+00],\n",
       "          [ 5.4054e-03, -8.1604e-02,  2.1899e-01,  ...,  3.1738e+00,\n",
       "           -3.1758e+00, -3.8184e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.8848e-01,  2.6538e-01,  3.1738e-01,  ...,  2.0000e+00,\n",
       "            2.7500e+00,  2.5469e+00],\n",
       "          [ 3.9844e-01,  3.2080e-01, -2.3401e-01,  ...,  2.6504e+00,\n",
       "           -9.9023e-01, -1.0760e-01]]]], dtype=torch.float16,\n",
       "       grad_fn=<ToCopyBackward0>), tensor([[[[-0.0051, -0.0068, -0.0297,  ...,  0.0829, -0.0443,  0.0043],\n",
       "          [-0.0497, -0.0100,  0.0115,  ...,  0.0665,  0.1100,  0.0352]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0707,  0.0389,  0.0705,  ..., -0.0340, -0.0146,  0.0341],\n",
       "          [-0.0101, -0.0426,  0.0237,  ...,  0.0865,  0.1495,  0.0192]]],\n",
       "\n",
       "\n",
       "        [[[-0.4517,  0.2106,  0.2209,  ..., -0.2474,  0.3567,  0.2166],\n",
       "          [ 0.0339,  0.3550,  0.0859,  ...,  0.0310,  0.0623, -0.0603]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0748, -0.6865,  0.8672,  ..., -0.2493,  0.3110, -0.5601],\n",
       "          [ 0.3232,  0.4927, -0.0166,  ..., -0.5234, -0.6016,  0.0450]]]],\n",
       "       dtype=torch.float16, grad_fn=<ToCopyBackward0>)), (tensor([[[[ 0.0275, -0.0743,  0.0041,  ...,  0.3022, -2.3379, -0.5000],\n",
       "          [-0.0766, -0.0524, -0.0706,  ..., -1.5322,  2.0605,  2.9785]]],\n",
       "\n",
       "\n",
       "        [[[-0.0251, -0.0664,  0.0409,  ..., -0.9849, -1.4951, -0.4680],\n",
       "          [-0.0257, -0.0268,  0.0660,  ...,  0.5513,  1.3711,  3.8027]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1580,  0.1326,  0.2566,  ..., -0.0292, -2.1875, -0.7461],\n",
       "          [ 0.4907,  0.1912, -0.0186,  ..., -0.4949,  1.8750,  0.8018]]],\n",
       "\n",
       "\n",
       "        [[[ 0.3655, -0.1410, -0.1750,  ...,  0.3765, -1.6787, -0.8892],\n",
       "          [ 0.3289,  0.1653,  0.7983,  ..., -0.5391,  0.2905,  1.4258]]]],\n",
       "       dtype=torch.float16, grad_fn=<ToCopyBackward0>), tensor([[[[ 0.0175,  0.0938, -0.0409,  ..., -0.0263,  0.0498, -0.0445],\n",
       "          [ 0.0704, -0.0875,  0.0403,  ...,  0.0495,  0.0931, -0.1058]]],\n",
       "\n",
       "\n",
       "        [[[-0.2263, -1.0840, -0.2443,  ...,  0.0278, -0.1135,  0.6289],\n",
       "          [ 0.0324,  0.0601, -0.0011,  ..., -0.0040, -0.0439,  0.0724]]],\n",
       "\n",
       "\n",
       "        [[[-0.1447, -0.1400, -0.4612,  ...,  0.2347, -0.5674,  0.0828],\n",
       "          [-0.1804,  0.2302, -0.3455,  ...,  0.1675, -0.4141,  0.0087]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1569, -0.3296, -0.4944,  ...,  0.4924, -0.5308, -0.1132],\n",
       "          [ 0.8130, -0.0441,  0.7490,  ..., -0.1411, -0.8467,  0.0622]]]],\n",
       "       dtype=torch.float16, grad_fn=<ToCopyBackward0>)), (tensor([[[[ 6.7078e-02,  2.1851e-02, -5.0751e-02,  ..., -6.1816e-01,\n",
       "            2.9707e+00,  4.6313e-01],\n",
       "          [-3.3661e-02, -1.1390e-04, -6.4636e-02,  ..., -2.0215e+00,\n",
       "            2.8442e-01, -1.6777e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 3.2776e-02,  1.0242e-01,  2.9251e-02,  ..., -2.3877e-01,\n",
       "            2.2383e+00,  2.2961e-01],\n",
       "          [-4.0192e-02,  4.5837e-02, -5.9387e-02,  ..., -2.5122e-01,\n",
       "            8.2324e-01, -1.7227e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 1.8958e-01, -1.5283e-01, -3.0103e-01,  ..., -5.7373e-01,\n",
       "            7.1973e-01, -6.0364e-02],\n",
       "          [ 1.9263e-01,  2.1326e-01,  2.0166e-01,  ...,  6.1475e-01,\n",
       "            6.2042e-02, -1.6191e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 4.1895e-01,  5.2002e-01, -7.1533e-01,  ...,  2.8516e-01,\n",
       "           -1.3672e+00, -1.5596e+00],\n",
       "          [ 3.3984e-01,  1.2445e-01, -2.4612e-02,  ..., -4.2822e-01,\n",
       "           -1.1982e+00, -2.3516e+00]]]], dtype=torch.float16,\n",
       "       grad_fn=<ToCopyBackward0>), tensor([[[[-0.0100,  0.0242, -0.0044,  ..., -0.0055,  0.0475, -0.0072],\n",
       "          [-0.0470, -0.1073, -0.0511,  ..., -0.0043, -0.0083, -0.0649]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0053, -0.0765,  0.0565,  ..., -0.0923,  0.1576, -0.0960],\n",
       "          [-0.1478, -0.0779, -0.0050,  ...,  0.0364, -0.0727, -0.2136]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0088,  0.3528, -0.0669,  ..., -0.0204, -0.0791, -0.0176],\n",
       "          [ 0.1438, -0.1782, -0.2198,  ..., -0.0587, -0.1313,  0.0102]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0352,  0.3811, -0.0993,  ...,  1.6143, -0.7563, -0.0821],\n",
       "          [ 0.6704, -0.1481,  0.3145,  ..., -0.0442, -0.2734,  0.2791]]]],\n",
       "       dtype=torch.float16, grad_fn=<ToCopyBackward0>)), (tensor([[[[ 0.0564, -0.0630, -0.0320,  ..., -1.1152, -1.9551, -0.2191],\n",
       "          [ 0.0457, -0.0310, -0.0458,  ...,  0.9326,  0.5137, -1.0586]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0375, -0.0406, -0.0227,  ..., -0.3220, -1.2080, -0.4805],\n",
       "          [-0.0734,  0.0284, -0.1255,  ..., -0.1337,  1.3271, -1.2715]]],\n",
       "\n",
       "\n",
       "        [[[-0.2487, -0.0367, -0.1254,  ..., -0.2908, -1.3164,  0.2905],\n",
       "          [-0.2881,  0.3494,  0.0506,  ..., -0.7944,  0.1376, -1.6074]]],\n",
       "\n",
       "\n",
       "        [[[ 0.5522, -0.4475,  0.0085,  ..., -1.8486, -3.3984,  3.0039],\n",
       "          [-0.2058,  0.3613,  0.3618,  ..., -1.0771,  0.9404, -3.0859]]]],\n",
       "       dtype=torch.float16, grad_fn=<ToCopyBackward0>), tensor([[[[-3.6865e-02,  7.7515e-02, -6.7200e-02,  ..., -5.1758e-02,\n",
       "            1.4294e-01, -5.1025e-02],\n",
       "          [ 5.1594e-04,  1.0626e-01, -1.3016e-02,  ...,  2.4612e-02,\n",
       "           -2.8610e-02,  5.9174e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 2.2400e-01, -2.5708e-01,  3.9771e-01,  ...,  7.5134e-02,\n",
       "           -4.8169e-01, -6.7383e-02],\n",
       "          [-2.3712e-02,  2.2736e-02,  1.8478e-02,  ...,  2.0187e-02,\n",
       "           -3.6774e-02, -4.1656e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 1.5405e-01, -2.7930e-01,  5.0781e-01,  ..., -2.4414e-01,\n",
       "            2.7420e-02,  1.0754e-01],\n",
       "          [ 3.8184e-01, -2.5342e-01, -3.3179e-01,  ...,  6.3171e-02,\n",
       "           -2.6221e-01,  2.9077e-01]]],\n",
       "\n",
       "\n",
       "        [[[-2.4004e+00, -6.4355e-01, -2.6123e-01,  ..., -8.0872e-02,\n",
       "            3.5278e-02, -9.2725e-01],\n",
       "          [ 5.3906e-01,  8.5742e-01, -3.3057e-01,  ...,  3.2324e-01,\n",
       "           -9.3262e-02, -3.9429e-01]]]], dtype=torch.float16,\n",
       "       grad_fn=<ToCopyBackward0>)), (tensor([[[[ 0.0685,  0.1095,  0.0672,  ..., -0.2556, -1.6162, -0.2712],\n",
       "          [-0.0484,  0.0488, -0.0961,  ...,  0.3047, -0.9951,  3.0312]]],\n",
       "\n",
       "\n",
       "        [[[-0.0737,  0.1139,  0.0392,  ..., -0.4070, -2.2344, -0.9639],\n",
       "          [-0.1138, -0.0253, -0.0912,  ...,  0.1002,  0.7051,  2.0723]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0503,  0.0686, -0.0823,  ...,  0.4026, -0.4026, -0.9375],\n",
       "          [-0.2468,  0.2954,  0.1136,  ..., -0.5005,  0.2520,  2.4375]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1847,  0.1436,  0.3293,  ..., -0.0591, -2.9551, -1.4551],\n",
       "          [ 0.1257,  0.1941, -0.4934,  ..., -1.3350,  0.7603,  1.9912]]]],\n",
       "       dtype=torch.float16, grad_fn=<ToCopyBackward0>), tensor([[[[-1.3969e-02, -2.0248e-02,  2.8744e-03,  ...,  8.4167e-02,\n",
       "           -5.2734e-02,  3.2368e-03],\n",
       "          [-1.4563e-01, -6.2622e-02,  2.6718e-02,  ...,  4.9347e-02,\n",
       "           -1.5053e-02, -6.2622e-02]]],\n",
       "\n",
       "\n",
       "        [[[-3.9764e-02,  2.6413e-02,  1.1810e-01,  ...,  6.5002e-03,\n",
       "            3.9062e-02, -3.3875e-02],\n",
       "          [ 4.4922e-01,  1.7432e-01, -1.0117e-02,  ..., -3.3234e-02,\n",
       "            4.0924e-02,  2.8223e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 2.0911e-01,  2.9236e-02,  4.7656e-01,  ...,  3.5889e-01,\n",
       "           -3.7231e-01, -4.0991e-01],\n",
       "          [ 7.4121e-01,  2.0276e-01, -9.1064e-02,  ..., -3.6890e-01,\n",
       "           -4.3433e-01,  2.8467e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 8.5596e-01,  6.4062e-01, -1.3330e-01,  ...,  1.6006e+00,\n",
       "           -5.3906e-01, -3.1699e+00],\n",
       "          [ 5.6152e-01,  8.4766e-01,  9.9316e-01,  ..., -1.4244e-02,\n",
       "           -1.9434e+00,  7.5830e-01]]]], dtype=torch.float16,\n",
       "       grad_fn=<ToCopyBackward0>)), (tensor([[[[-0.0021,  0.0431, -0.0384,  ...,  0.7339, -0.7632,  1.4229],\n",
       "          [-0.0317, -0.0853,  0.0602,  ..., -0.1196, -1.7822, -0.7612]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0260, -0.0367,  0.0815,  ...,  1.2080, -0.3181,  1.4434],\n",
       "          [-0.0124, -0.0275,  0.0516,  ..., -0.1296, -1.6953, -0.6948]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0793, -0.2211, -0.1332,  ...,  0.7339,  0.1001,  0.9629],\n",
       "          [ 0.1570, -0.2076, -0.3350,  ..., -0.6172, -1.0898, -0.4221]]],\n",
       "\n",
       "\n",
       "        [[[-0.0105, -0.4531, -0.6934,  ..., -0.6318,  0.6943,  0.4204],\n",
       "          [ 0.1030,  0.1125, -0.0787,  ..., -0.6963, -0.2419, -0.4299]]]],\n",
       "       dtype=torch.float16, grad_fn=<ToCopyBackward0>), tensor([[[[ 1.6159e-02,  2.7512e-02, -1.4931e-02,  ...,  1.0461e-01,\n",
       "            8.1909e-02,  6.2408e-02],\n",
       "          [-5.9143e-02,  5.5115e-02, -4.7150e-03,  ..., -2.4017e-02,\n",
       "           -2.7990e-04,  2.4399e-02]]],\n",
       "\n",
       "\n",
       "        [[[-1.1493e-01, -4.0771e-02,  4.4769e-02,  ..., -1.8933e-01,\n",
       "           -3.3112e-02, -1.0223e-01],\n",
       "          [ 5.7495e-02, -6.9275e-02, -1.1131e-02,  ..., -1.1407e-01,\n",
       "           -4.0253e-02, -1.0577e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 5.8502e-02, -2.4292e-01, -4.0576e-01,  ...,  6.4941e-01,\n",
       "            6.1523e-01, -3.3325e-01],\n",
       "          [-5.2338e-02,  1.1115e-01,  1.0681e-01,  ..., -4.7028e-02,\n",
       "            3.0811e-01, -1.0455e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.3684e-01, -5.3125e-01,  1.8311e+00,  ...,  2.2559e+00,\n",
       "            6.7334e-01, -1.4316e+00],\n",
       "          [-5.0586e-01,  2.8662e-01, -7.1582e-01,  ...,  1.1237e-01,\n",
       "            2.0938e+00, -2.0618e-01]]]], dtype=torch.float16,\n",
       "       grad_fn=<ToCopyBackward0>)), (tensor([[[[ 4.1351e-03, -5.5351e-03,  3.3112e-02,  ...,  8.0811e-02,\n",
       "           -2.0520e-01,  8.4375e-01],\n",
       "          [ 1.7670e-02, -1.6098e-02,  1.2407e-03,  ..., -6.6467e-02,\n",
       "            9.6143e-01,  2.2552e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 9.9945e-03,  8.1482e-02,  9.6497e-02,  ..., -7.8076e-01,\n",
       "           -1.0657e-01, -5.0146e-01],\n",
       "          [-1.4172e-01,  1.0144e-01, -4.6661e-02,  ..., -1.9714e-01,\n",
       "            1.4248e+00,  1.2373e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 2.5732e-01, -4.4165e-01, -2.5659e-01,  ..., -8.3496e-01,\n",
       "           -1.2103e-01, -7.2021e-01],\n",
       "          [ 5.2441e-01,  5.9229e-01, -1.4209e-01,  ...,  3.3081e-01,\n",
       "            3.0103e-01,  3.8013e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 2.1277e-01,  2.4377e-01,  8.5815e-02,  ..., -2.3887e+00,\n",
       "           -7.4561e-01, -1.4414e+00],\n",
       "          [-4.4482e-01,  5.8594e-01, -9.6008e-02,  ..., -2.1704e-01,\n",
       "            1.6104e+00,  2.2812e+00]]]], dtype=torch.float16,\n",
       "       grad_fn=<ToCopyBackward0>), tensor([[[[-5.2612e-02, -1.0002e-02,  2.6321e-03,  ...,  2.2690e-02,\n",
       "            3.6469e-02,  1.3474e-02],\n",
       "          [-2.6993e-02, -2.8275e-02, -4.4708e-03,  ..., -8.3069e-02,\n",
       "            2.5665e-02,  4.0497e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 3.2368e-03,  4.7913e-03, -6.3660e-02,  ...,  6.8617e-04,\n",
       "            1.4221e-01,  1.7273e-01],\n",
       "          [ 7.0618e-02,  6.8909e-02,  8.6365e-02,  ...,  9.0149e-02,\n",
       "           -5.4962e-02, -1.4397e-02]]],\n",
       "\n",
       "\n",
       "        [[[-7.5537e-01,  7.8430e-02,  1.1597e-01,  ...,  5.9326e-01,\n",
       "           -3.5742e-01, -3.4424e-01],\n",
       "          [ 2.4438e-01, -2.0093e-01, -9.0881e-02,  ...,  3.2593e-01,\n",
       "           -2.6367e-01, -8.5678e-03]]],\n",
       "\n",
       "\n",
       "        [[[-2.4434e+00, -7.1924e-01, -9.2334e-01,  ...,  5.6055e-01,\n",
       "            5.2100e-01,  7.6270e-01],\n",
       "          [ 1.9287e+00, -8.8135e-01,  1.8494e-01,  ...,  4.8120e-01,\n",
       "           -5.5029e-01, -1.7297e-01]]]], dtype=torch.float16,\n",
       "       grad_fn=<ToCopyBackward0>)), (tensor([[[[-1.1200e-01, -2.7771e-02, -8.4595e-02,  ...,  3.0420e-01,\n",
       "            4.7583e-01,  1.2070e+00],\n",
       "          [-4.8859e-02, -4.7485e-02,  4.7974e-02,  ...,  2.6538e-01,\n",
       "            4.3726e-01,  4.8682e-01]]],\n",
       "\n",
       "\n",
       "        [[[-5.0201e-02, -1.2170e-01,  1.5175e-02,  ..., -2.0764e-01,\n",
       "            2.7246e-01,  2.7271e-01],\n",
       "          [ 1.8585e-02, -5.4535e-02, -2.1744e-02,  ..., -1.6809e-01,\n",
       "           -1.2537e-01,  1.4580e+00]]],\n",
       "\n",
       "\n",
       "        [[[-1.5521e-04,  1.7542e-01,  2.1729e-01,  ..., -3.9185e-01,\n",
       "           -8.9722e-02, -4.0771e-01],\n",
       "          [-2.0349e-01,  2.9053e-01,  5.4639e-01,  ..., -9.8145e-01,\n",
       "            6.0449e-01,  2.3547e-01]]],\n",
       "\n",
       "\n",
       "        [[[-4.0405e-01,  2.6904e-01,  1.6150e-01,  ..., -1.3660e-01,\n",
       "           -1.4062e+00, -1.3672e+00],\n",
       "          [-6.8799e-01, -1.5698e-01,  9.9512e-01,  ...,  6.7773e-01,\n",
       "            1.0234e+00,  7.1826e-01]]]], dtype=torch.float16,\n",
       "       grad_fn=<ToCopyBackward0>), tensor([[[[ 4.4037e-02,  1.1742e-02, -8.6899e-03,  ...,  4.6997e-02,\n",
       "           -1.6165e-03, -7.4036e-02],\n",
       "          [ 6.6948e-03,  5.4970e-03,  8.2703e-03,  ...,  2.1515e-02,\n",
       "            1.1108e-02, -2.3666e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 3.6133e-02, -1.4946e-02,  4.0527e-02,  ..., -4.0802e-02,\n",
       "            9.5337e-02,  5.4504e-02],\n",
       "          [-4.6417e-02,  5.7404e-02, -3.3691e-01,  ...,  7.4707e-02,\n",
       "            5.7373e-02,  5.6887e-04]]],\n",
       "\n",
       "\n",
       "        [[[-5.6055e-01, -7.0752e-01, -6.5186e-01,  ..., -9.6558e-02,\n",
       "            4.2505e-01,  2.5488e-01],\n",
       "          [ 1.8188e-01, -4.3945e-01,  4.8267e-01,  ..., -6.1493e-02,\n",
       "            3.8281e-01, -8.4473e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.2051e+00, -4.3262e-01, -2.8125e-01,  ...,  5.8447e-01,\n",
       "           -5.8154e-01, -5.4932e-02],\n",
       "          [-1.0645e+00, -5.5713e-01, -6.7676e-01,  ...,  1.4531e+00,\n",
       "            2.2598e+00, -7.1240e-01]]]], dtype=torch.float16,\n",
       "       grad_fn=<ToCopyBackward0>)), (tensor([[[[-4.7913e-03,  3.4393e-02, -6.4148e-02,  ...,  1.5830e+00,\n",
       "            1.3076e+00, -7.6758e+00],\n",
       "          [-4.4312e-02, -2.1439e-02,  2.9663e-02,  ...,  2.5940e-02,\n",
       "            6.5137e-01, -3.7964e-02]]],\n",
       "\n",
       "\n",
       "        [[[-8.0627e-02, -8.8135e-02, -7.0419e-03,  ...,  7.2705e-01,\n",
       "            1.3398e+00, -7.4883e+00],\n",
       "          [-8.4412e-02,  2.6840e-02,  5.5939e-02,  ..., -2.0850e-01,\n",
       "            4.9512e-01,  2.4780e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 2.4536e-01,  6.9275e-02,  1.8799e-02,  ...,  1.4221e-01,\n",
       "            1.0332e+00, -4.8906e+00],\n",
       "          [-3.9282e-01,  9.0027e-03,  4.1016e-01,  ...,  8.4473e-01,\n",
       "            6.2256e-01,  5.4297e-01]]],\n",
       "\n",
       "\n",
       "        [[[-5.4382e-02, -4.9536e-01,  1.3745e-01,  ...,  2.6250e+00,\n",
       "           -3.1921e-02, -3.2500e+00],\n",
       "          [ 6.0059e-01,  1.2073e-01,  9.1675e-02,  ...,  1.5117e+00,\n",
       "           -5.6641e-01, -3.0859e+00]]]], dtype=torch.float16,\n",
       "       grad_fn=<ToCopyBackward0>), tensor([[[[-4.8431e-02,  8.8074e-02, -8.5693e-02,  ..., -2.4689e-02,\n",
       "            6.7932e-02, -9.9854e-02],\n",
       "          [-3.4943e-02, -5.7739e-02, -3.6560e-02,  ..., -5.0232e-02,\n",
       "           -3.1281e-02,  2.2221e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 1.5688e-03, -1.3708e-01,  1.8860e-01,  ...,  1.3989e-01,\n",
       "            4.1656e-02,  5.4541e-01],\n",
       "          [-3.0685e-02,  7.9041e-02, -1.5466e-01,  ...,  1.1420e-01,\n",
       "           -7.9834e-02,  1.9531e-03]]],\n",
       "\n",
       "\n",
       "        [[[-3.9600e-01,  3.5791e-01,  4.8071e-01,  ...,  2.1997e-01,\n",
       "           -5.6213e-02,  1.8433e-01],\n",
       "          [-3.2739e-01,  3.4082e-01, -4.1992e-01,  ..., -4.4128e-02,\n",
       "           -7.8223e-01,  3.9844e-01]]],\n",
       "\n",
       "\n",
       "        [[[-3.1519e-01, -7.0557e-01,  4.4409e-01,  ..., -2.4395e+00,\n",
       "           -1.3193e+00, -1.4883e+00],\n",
       "          [-1.3730e+00, -4.3750e+00,  1.5918e-01,  ...,  8.4326e-01,\n",
       "            3.4805e+00,  3.3301e+00]]]], dtype=torch.float16,\n",
       "       grad_fn=<ToCopyBackward0>)), (tensor([[[[-0.0147,  0.0818, -0.0136,  ...,  0.3247,  0.1733, -0.6523],\n",
       "          [-0.1189, -0.1049,  0.0565,  ..., -0.1292,  1.3047, -0.2888]]],\n",
       "\n",
       "\n",
       "        [[[-0.0263,  0.1697,  0.1151,  ..., -0.3667,  0.8179,  0.5498],\n",
       "          [-0.0044, -0.1787,  0.1279,  ..., -0.9541,  0.9204,  0.0233]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1292,  0.1039, -0.2544,  ...,  0.5527,  0.2473, -0.3743],\n",
       "          [-0.1412,  0.1646,  0.0424,  ..., -1.7754,  1.0244,  1.0342]]],\n",
       "\n",
       "\n",
       "        [[[-0.1891,  0.8608, -0.6938,  ..., -1.8135,  0.6396,  0.5190],\n",
       "          [-0.1129, -0.4131, -0.6143,  ..., -2.9297, -0.4333,  3.3242]]]],\n",
       "       dtype=torch.float16, grad_fn=<ToCopyBackward0>), tensor([[[[ 7.8003e-02, -1.1269e-02,  9.2102e-02,  ...,  1.7932e-01,\n",
       "            3.4882e-02,  1.0042e-03],\n",
       "          [ 3.9886e-02, -1.4008e-02, -3.5858e-02,  ..., -6.1340e-02,\n",
       "            5.1514e-02, -3.1189e-02]]],\n",
       "\n",
       "\n",
       "        [[[-1.3171e-01,  1.6528e-01, -2.6050e-01,  ..., -3.3838e-01,\n",
       "            1.6223e-01,  1.2274e-01],\n",
       "          [-1.9153e-01,  2.3511e-01,  9.5093e-02,  ...,  7.8552e-02,\n",
       "           -2.4704e-02, -4.1504e-02]]],\n",
       "\n",
       "\n",
       "        [[[-5.1074e-01,  3.8184e-01, -4.6875e-01,  ..., -6.5088e-01,\n",
       "            1.2396e-01, -3.9941e-01],\n",
       "          [ 2.7222e-01,  3.6206e-01,  7.2559e-01,  ...,  1.8518e-01,\n",
       "           -5.0879e-01,  2.8271e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.2347e-01,  1.6614e-01,  7.9736e-01,  ..., -7.6416e-01,\n",
       "           -9.9976e-02, -1.7959e+00],\n",
       "          [ 5.1172e-01, -2.0723e+00, -4.8535e-01,  ...,  2.4048e-01,\n",
       "           -2.1719e+00, -1.8372e-01]]]], dtype=torch.float16,\n",
       "       grad_fn=<ToCopyBackward0>)), (tensor([[[[-0.0710,  0.0907, -0.0717,  ...,  0.5029,  0.2032,  0.1569],\n",
       "          [-0.0419, -0.0712, -0.0031,  ..., -0.9648,  0.3867, -0.2944]]],\n",
       "\n",
       "\n",
       "        [[[-0.0125, -0.1610,  0.0149,  ..., -1.2939, -0.6245,  0.9258],\n",
       "          [-0.0346, -0.0557, -0.0212,  ..., -0.0939,  0.5098,  0.9668]]],\n",
       "\n",
       "\n",
       "        [[[-0.4465,  0.1747, -0.0281,  ..., -1.7217, -0.2018,  0.1477],\n",
       "          [-0.3020, -0.1782,  0.1281,  ..., -0.2261,  0.2452,  0.9565]]],\n",
       "\n",
       "\n",
       "        [[[-0.9595, -0.4478, -0.2766,  ..., -1.7129,  0.2316, -0.3091],\n",
       "          [ 0.4194, -0.8438,  0.7739,  ..., -2.0312,  2.7539,  0.4917]]]],\n",
       "       dtype=torch.float16, grad_fn=<ToCopyBackward0>), tensor([[[[ 0.0056,  0.0158,  0.0192,  ..., -0.0461,  0.0041, -0.0771],\n",
       "          [-0.0340,  0.1166, -0.0240,  ..., -0.1536,  0.1445,  0.0296]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0222,  0.3325, -0.1494,  ...,  0.1117,  0.0359, -0.0742],\n",
       "          [ 0.0669, -0.1471,  0.1294,  ...,  0.0621, -0.1104, -0.0238]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0606, -0.9678, -0.4956,  ..., -0.4990, -0.3999, -0.0305],\n",
       "          [ 0.3997,  0.1525, -0.1392,  ...,  0.5381, -0.2156, -0.0126]]],\n",
       "\n",
       "\n",
       "        [[[ 0.4521, -0.5444, -0.1542,  ..., -0.4294, -0.1860, -1.2783],\n",
       "          [ 0.4121, -1.0488, -0.0192,  ...,  2.8945, -1.1113, -2.2871]]]],\n",
       "       dtype=torch.float16, grad_fn=<ToCopyBackward0>)), (tensor([[[[-0.0374,  0.0607, -0.0652,  ...,  0.5181, -0.5825, -0.3577],\n",
       "          [-0.0206, -0.0349, -0.0610,  ..., -0.2693, -0.4087, -0.8081]]],\n",
       "\n",
       "\n",
       "        [[[-0.0948,  0.0394, -0.0791,  ...,  0.0311,  0.7788, -0.3884],\n",
       "          [-0.0601, -0.0630, -0.0330,  ...,  0.6235, -0.3079, -1.0117]]],\n",
       "\n",
       "\n",
       "        [[[-0.4558, -0.6392, -0.3054,  ...,  0.5371,  0.4451,  0.1638],\n",
       "          [-0.1571, -0.1906, -0.0756,  ...,  0.3687, -0.4717, -0.7461]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1167,  0.3787, -1.5498,  ...,  2.1445, -0.5938,  2.0254],\n",
       "          [ 0.0891,  0.0343,  0.3975,  ...,  2.0918, -1.5293,  0.5044]]]],\n",
       "       dtype=torch.float16, grad_fn=<ToCopyBackward0>), tensor([[[[-9.7885e-03,  8.0322e-02, -4.7722e-03,  ..., -2.3148e-02,\n",
       "           -1.6037e-02,  4.3030e-02],\n",
       "          [ 2.2934e-02,  4.4067e-02, -8.7967e-03,  ..., -8.0383e-02,\n",
       "            2.3723e-04,  3.1555e-02]]],\n",
       "\n",
       "\n",
       "        [[[-1.8079e-01,  3.8300e-02, -3.3630e-02,  ..., -1.3379e-01,\n",
       "            2.7954e-01,  5.3986e-02],\n",
       "          [ 1.9852e-02,  7.0435e-02, -2.1133e-02,  ..., -2.8107e-02,\n",
       "            4.2090e-01,  6.1401e-02]]],\n",
       "\n",
       "\n",
       "        [[[-3.4155e-01,  6.3916e-01,  1.4233e-01,  ..., -4.1199e-02,\n",
       "           -7.0020e-01,  8.0566e-01],\n",
       "          [ 1.4531e+00,  2.0059e+00, -1.0869e+00,  ..., -2.4646e-01,\n",
       "           -1.3613e+00, -4.7339e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.7178e+00, -4.6460e-01,  1.5420e+00,  ...,  2.4268e-01,\n",
       "            1.1895e+00,  6.6797e-01],\n",
       "          [ 1.6260e+00,  5.4609e+00,  1.6826e+00,  ..., -7.7588e-01,\n",
       "           -2.1252e-01, -8.2642e-02]]]], dtype=torch.float16,\n",
       "       grad_fn=<ToCopyBackward0>)), (tensor([[[[-0.0723, -0.0164,  0.0063,  ..., -1.7207,  0.6973,  0.0472],\n",
       "          [ 0.0442,  0.1133,  0.0525,  ..., -1.8643,  0.2013,  0.3494]]],\n",
       "\n",
       "\n",
       "        [[[-0.0446, -0.1007,  0.0217,  ..., -0.4814,  0.3374,  0.2612],\n",
       "          [-0.0847,  0.1042, -0.1312,  ..., -1.3369,  0.2502,  0.0823]]],\n",
       "\n",
       "\n",
       "        [[[-0.1827,  0.0537, -0.1039,  ...,  0.3892,  0.9346,  0.5083],\n",
       "          [-0.0234,  0.1229,  0.0794,  ..., -0.6294, -0.7534,  0.2183]]],\n",
       "\n",
       "\n",
       "        [[[-0.5454,  0.0900, -0.2783,  ...,  0.2871,  1.7988, -2.6641],\n",
       "          [ 0.1857, -0.1716,  0.3079,  ..., -1.4141, -1.3662, -1.2070]]]],\n",
       "       dtype=torch.float16, grad_fn=<ToCopyBackward0>), tensor([[[[-0.0572, -0.0098,  0.0266,  ..., -0.0765, -0.0951, -0.0657],\n",
       "          [-0.0048,  0.0675,  0.1846,  ...,  0.0129,  0.1753, -0.1246]]],\n",
       "\n",
       "\n",
       "        [[[-0.0224, -0.0801, -0.0226,  ..., -0.0312,  0.0942,  0.1438],\n",
       "          [-0.2198, -0.3679, -0.6709,  ..., -0.0972, -0.2634,  0.0692]]],\n",
       "\n",
       "\n",
       "        [[[-0.6841, -0.6313,  0.7896,  ..., -0.2361, -0.1678, -0.3250],\n",
       "          [ 0.0764, -0.5879, -0.2333,  ..., -0.1899, -0.2135, -0.6338]]],\n",
       "\n",
       "\n",
       "        [[[ 1.6094, -0.1757,  1.6133,  ...,  1.6650,  1.6006, -3.4062],\n",
       "          [ 0.2383,  1.1006,  0.9248,  ..., -0.6953, -0.4055, -2.7500]]]],\n",
       "       dtype=torch.float16, grad_fn=<ToCopyBackward0>)), (tensor([[[[ 0.0154, -0.0741, -0.0102,  ...,  0.2058,  2.4453,  1.3848],\n",
       "          [ 0.0814,  0.0113,  0.0418,  ..., -0.2935, -0.6191,  0.3115]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0134,  0.0308,  0.0403,  ..., -1.0244,  1.9766,  1.9609],\n",
       "          [ 0.1390,  0.0734,  0.0477,  ..., -1.1455, -0.4563, -0.4929]]],\n",
       "\n",
       "\n",
       "        [[[ 0.2291, -0.2529, -0.0397,  ..., -0.7119,  0.2944,  2.2812],\n",
       "          [-0.2581, -0.4043,  0.2198,  ..., -0.5332,  0.1897, -0.0660]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1901,  0.4255,  0.6670,  ..., -3.0176, -1.2139,  0.9458],\n",
       "          [ 0.2803, -0.4082, -0.3733,  ..., -0.9746, -1.4775, -1.4717]]]],\n",
       "       dtype=torch.float16, grad_fn=<ToCopyBackward0>), tensor([[[[ 2.2903e-02,  1.6479e-02, -1.3220e-01,  ...,  6.2164e-02,\n",
       "           -5.4779e-02,  5.2979e-02],\n",
       "          [-1.1435e-03, -4.4670e-03,  3.8177e-02,  ..., -1.3466e-02,\n",
       "           -1.1359e-01,  7.6675e-03]]],\n",
       "\n",
       "\n",
       "        [[[-5.0995e-02, -3.4912e-02,  1.7200e-01,  ..., -9.2041e-02,\n",
       "           -1.5991e-01,  1.1554e-01],\n",
       "          [ 1.4209e-01, -2.9968e-02, -1.1911e-03,  ..., -2.7603e-02,\n",
       "           -7.9346e-02,  2.5055e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 4.7241e-01, -2.1704e-01,  3.6255e-01,  ...,  2.9150e-01,\n",
       "            6.5804e-03, -5.4436e-03],\n",
       "          [ 4.7827e-01,  5.1025e-01, -6.2744e-01,  ...,  7.5928e-02,\n",
       "           -8.6243e-02, -7.2559e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 2.6309e+00, -9.6045e-01,  1.1064e+00,  ..., -1.0615e+00,\n",
       "            8.0859e-01,  1.0664e+00],\n",
       "          [-2.5820e+00,  8.0322e-01, -3.2754e+00,  ...,  8.3643e-01,\n",
       "           -1.5791e+00, -9.0723e-01]]]], dtype=torch.float16,\n",
       "       grad_fn=<ToCopyBackward0>)), (tensor([[[[-1.8936e-02,  8.9722e-03, -7.2266e-02,  ...,  1.5469e+00,\n",
       "           -7.2021e-01, -5.2100e-01],\n",
       "          [ 1.2178e-03,  6.1340e-02,  4.3449e-03,  ...,  6.3428e-01,\n",
       "            7.2344e+00,  8.4521e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 7.0007e-02, -3.8574e-02, -6.4087e-02,  ...,  1.6494e+00,\n",
       "           -1.2969e+00, -2.9907e-01],\n",
       "          [-4.6967e-02,  3.5004e-02, -1.1108e-01,  ...,  1.3379e+00,\n",
       "            7.1797e+00,  2.1179e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 1.8967e-02,  9.8511e-02, -9.9426e-02,  ...,  1.9141e+00,\n",
       "           -6.2109e-01, -8.3740e-01],\n",
       "          [-6.5479e-01,  2.7002e-01,  5.8838e-01,  ...,  4.8438e-01,\n",
       "            8.5156e-01,  1.1729e+00]]],\n",
       "\n",
       "\n",
       "        [[[ 1.4087e-01, -7.3340e-01,  4.7754e-01,  ...,  1.4185e-01,\n",
       "           -2.0078e+00, -4.7485e-02],\n",
       "          [-6.5234e-01,  1.3147e-01,  1.3457e+00,  ..., -4.2603e-01,\n",
       "           -4.2539e+00, -4.8901e-01]]]], dtype=torch.float16,\n",
       "       grad_fn=<ToCopyBackward0>), tensor([[[[ 1.6675e-01, -1.3635e-01,  4.8798e-02,  ..., -1.3281e-01,\n",
       "            1.0773e-01, -1.3074e-01],\n",
       "          [-5.7068e-02, -2.7603e-02, -3.6652e-02,  ...,  4.9377e-02,\n",
       "           -2.0218e-03,  2.8076e-02]]],\n",
       "\n",
       "\n",
       "        [[[-3.2397e-01,  3.7842e-01, -3.1396e-01,  ...,  2.4280e-01,\n",
       "           -2.9858e-01,  3.9551e-01],\n",
       "          [-9.3536e-03, -3.9154e-02,  1.1609e-01,  ...,  8.2031e-02,\n",
       "            1.0139e-02,  1.9547e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 6.6956e-02, -3.5522e-01,  3.9111e-01,  ...,  3.8501e-01,\n",
       "            6.0742e-01, -6.1279e-01],\n",
       "          [-1.2329e-01, -3.1128e-01, -1.5723e+00,  ...,  4.3488e-02,\n",
       "           -9.9951e-01,  4.1699e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 3.2788e-01,  1.0244e+00,  2.6152e+00,  ..., -1.0283e+00,\n",
       "           -6.3574e-01, -1.5225e+00],\n",
       "          [-1.5791e+00, -1.9863e+00, -1.8594e+00,  ..., -1.2529e+00,\n",
       "           -1.7969e-01,  4.1821e-01]]]], dtype=torch.float16,\n",
       "       grad_fn=<ToCopyBackward0>)), (tensor([[[[-0.0055, -0.0689, -0.1050,  ...,  0.2295,  0.6650, -0.1051],\n",
       "          [-0.0284, -0.0578,  0.0240,  ..., -1.8945,  1.0244,  0.0585]]],\n",
       "\n",
       "\n",
       "        [[[-0.0101, -0.0600, -0.0340,  ..., -1.4404, -0.2202,  0.1067],\n",
       "          [-0.0281, -0.0848, -0.0893,  ..., -1.0615,  1.1123, -0.4041]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1213, -0.6362, -0.0928,  ...,  1.0293, -0.2754,  0.4431],\n",
       "          [ 0.8921, -0.5879, -0.1293,  ..., -1.2461, -0.0607, -1.1465]]],\n",
       "\n",
       "\n",
       "        [[[ 0.1816, -0.2549, -0.6899,  ..., -0.3594, -2.4375, -0.8306],\n",
       "          [ 1.1152,  1.3242, -0.9946,  ..., -0.4929, -0.1208,  2.7051]]]],\n",
       "       dtype=torch.float16, grad_fn=<ToCopyBackward0>), tensor([[[[ 0.1058, -0.1808,  0.3022,  ...,  0.1320,  0.3325,  0.3167],\n",
       "          [ 0.1758, -0.0504,  0.0227,  ..., -0.2964, -0.3081, -0.5029]]],\n",
       "\n",
       "\n",
       "        [[[-0.0967,  0.6274, -1.5742,  ..., -0.4707, -0.7402, -0.9946],\n",
       "          [-0.2705, -0.0739, -0.0163,  ...,  0.4663,  0.2255,  0.4224]]],\n",
       "\n",
       "\n",
       "        [[[-0.5874,  0.0060,  0.2207,  ..., -0.5425,  0.2371, -0.8481],\n",
       "          [ 0.6333, -0.5938, -0.3384,  ...,  0.5586,  0.0802,  0.5454]]],\n",
       "\n",
       "\n",
       "        [[[-1.3066,  0.4316, -0.3062,  ...,  0.7534, -1.5068, -1.1230],\n",
       "          [-0.3767, -0.0199, -0.0687,  ..., -0.2505,  0.3687,  0.1749]]]],\n",
       "       dtype=torch.float16, grad_fn=<ToCopyBackward0>))), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = res['input_ids'].clone()\n",
    "model(input_ids=res['input_ids'],labels=labels)\n",
    "\n",
    "# model_glm = model_glm.half()\n",
    "# labels_glm = res_glm['input_ids'].clone()\n",
    "# model_glm(input_ids=res_glm['input_ids'],labels=labels_glm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**对比一下 ChatGLM 跟 ChatGLM2 的结构差别：**\n",
    "\n",
    "ChatGLM:\n",
    "```python\n",
    "ChatGLMForConditionalGeneration(\n",
    "  (transformer): ChatGLMModel(\n",
    "    (word_embeddings): Embedding(130528, 4096)\n",
    "    (layers): ModuleList(\n",
    "      (0-27): 28 x GLMBlock(\n",
    "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
    "        (attention): SelfAttention(\n",
    "          (rotary_emb): RotaryEmbedding()\n",
    "          (query_key_value): Linear(in_features=4096, out_features=12288, bias=True)\n",
    "          (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
    "        )\n",
    "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
    "        (mlp): GLU(\n",
    "          (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
    "          (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "    (final_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
    "  )\n",
    "  (lm_head): Linear(in_features=4096, out_features=130528, bias=False)\n",
    "```\n",
    "\n",
    "ChatGLM2:\n",
    "```python\n",
    "ChatGLMForConditionalGeneration(\n",
    "  (transformer): ChatGLMModel(\n",
    "    (embedding): Embedding(\n",
    "      (word_embeddings): Embedding(65024, 4096)   # <-- smaller vocab size\n",
    "    )\n",
    "    (rotary_pos_emb): RotaryEmbedding()\n",
    "    (encoder): GLMTransformer(\n",
    "      (layers): ModuleList(\n",
    "        (0-27): 28 x GLMBlock(\n",
    "          (input_layernorm): RMSNorm()   # <-- LayerNorm to RMSNorm\n",
    "          (self_attention): SelfAttention(\n",
    "            (query_key_value): Linear(in_features=4096, out_features=4608, bias=True)   # <-- smaller attention out_features\n",
    "            (core_attention): CoreAttention(\n",
    "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
    "            )\n",
    "            (dense): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "          )\n",
    "          (post_attention_layernorm): RMSNorm()\n",
    "          (mlp): MLP(                # <-- GLU to MLP\n",
    "            (dense_h_to_4h): Linear(in_features=4096, out_features=27392, bias=False)\n",
    "            (dense_4h_to_h): Linear(in_features=13696, out_features=4096, bias=False)\n",
    "          )\n",
    "        )\n",
    "      )\n",
    "      (final_layernorm): RMSNorm()\n",
    "    )\n",
    "    (output_layer): Linear(in_features=4096, out_features=65024, bias=False)    # <-- smaller out_features\n",
    "  )\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=4096, out_features=130528, bias=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_glm.lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=4096, out_features=65024, bias=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"ORG\": \"ChatGPT\", \"sentiment\": \"消极\", \"reason\": \"ChatGPT的提出对谷嘎、万度的搜索业务产生巨大打击，传统搜索引擎的作用性降低了。\"},\n",
      "{\"ORG\": \"OChat\", \"sentiment\": \"积极\", \"reason\": \"OChat，Linguo等新兴语义搜索公司，迅速推出自己的类ChatGPT模型，并结合进自家搜索引擎，受到了很多用户的青睐。\"},\n",
      "{\"ORG\": \"腾势\", \"sentiment\": \"积极\", \"reason\": \"腾势、艾里等公司表示会迅速跟进ChatGPT和AIGC的发展，并预计在年底前推出自己的大模型。\"},\n",
      "{\"ORG\": \"视觉中国\", \"sentiment\": \"中性\", \"reason\": \"大型图片供应商视觉中国称ChatGPT对公司业务暂无影响，还在观望状态。\"},\n",
      "{\"ORG\": \"亚牛逊公司\", \"sentiment\": \"中性\", \"reason\": \"亚牛逊公司关于AIGC的表态中并未提及ChatGPT对公司业务的影响。\"},\n",
      "{\"ORG\": \"巨硬公司\", \"sentiment\": \"中性\", \"reason\": \"巨硬公司昨日在A股上市与ChatGPT对公司业务的影响无关。\"}\n"
     ]
    }
   ],
   "source": [
    "print(model.chat(tokenizer,query=text)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /root/anaconda3/envs/gby/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda121.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 121\n",
      "CUDA SETUP: Loading binary /root/anaconda3/envs/gby/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda121.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/gby/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /root/anaconda3/envs/gby did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/root/anaconda3/envs/gby/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/root/anaconda3/envs/gby/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(model, \"weights/sentiment_comp_ie_chatglm2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"ORG\": \"谷嘎\", \"sentiment\": \"消极\", \"reason\": \"ChatGPT的提出对其搜索业务产生巨大打击\"}\n",
      "{\"ORG\": \"万度\", \"sentiment\": \"消极\", \"reason\": \"传统搜索引擎的作用性降低了\"}\n",
      "{\"ORG\": \"OChat\", \"sentiment\": \"积极\", \"reason\": \"迅速推出自己的类ChatGPT模型，并结合进自家搜索引擎，受到了很多用户的青睐\"}\n",
      "{\"ORG\": \"Linguo\", \"sentiment\": \"积极\", \"reason\": \"迅速推出自己的类ChatGPT模型，并结合进自家搜索引擎，受到了很多用户的青睐\"}\n",
      "{\"ORG\": \"腾势\", \"sentiment\": \"积极\", \"reason\": \"会迅速跟进ChatGPT和AIGC的发展，并预计在年底前推出自己的大模型\"}\n",
      "{\"ORG\": \"艾里\", \"sentiment\": \"积极\", \"reason\": \"会迅速跟进ChatGPT和AIGC的发展，并预计在年底前推出自己的大模型\"}\n",
      "{\"ORG\": \"视觉中国\", \"sentiment\": \"中性\", \"reason\": \"称ChatGPT对公司业务暂无影响，还在观望状态\"}\n"
     ]
    }
   ],
   "source": [
    "print(model.chat(tokenizer,text)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gby",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
